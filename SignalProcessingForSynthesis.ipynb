{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SignalProcessingForSynthesis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djsabelo/BiosignalsDeepLearningWorkshop/blob/main/SignalProcessingForSynthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1y1YPqkXIiH"
      },
      "source": [
        "Processing:\r\n",
        "\r\n",
        "1. Sacar ficheiro - meter em numpy arrays\r\n",
        "2. Fazer plot aos dados com dois ou mais sinais\r\n",
        "3. Cortar os dados\r\n",
        "4. Remover a média e normalizar com o maximo absoluto\r\n",
        "5. Subamostragem (decimate)\r\n",
        "5. Fazer plot aos dados com dois ou mais sinais\r\n",
        "6. Remover ruido - smooth\r\n",
        "7. Fazer plot aos dados\r\n",
        "8. Mostrar e tratar do baseline wander\r\n",
        "9. Remover o minimo\r\n",
        "10. Quantização\r\n",
        "11. Fazer plot aos dados\r\n",
        "12. Segmentação - tens que garantir pelo menos 2 ciclos - sliding window (janelas 2^x + 1)\r\n",
        "11. Fazer plot de muitas janelas\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI7mzTAERpq7"
      },
      "source": [
        "# Signal Processing for  Synthesis\r\n",
        "\r\n",
        "The application of Deep Learning for biosignals synthesis depends on the quality of signal processing and even on the quality of the raw signal.\r\n",
        "\r\n",
        "In this workshop, we will go through the signal processing steps that are usually applied for biosignals synthesis using Python. Here, we will use **numpy** to ease mathematical operations, **matplotlib** to visualize the results of each step and **os** to search data files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_-lNo7SU3xy"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib as plt\r\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzvWIDYnXqKz",
        "outputId": "c6bcab87-4352-4226-8fbf-9d3e1f735044",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/djsabelo/BiosignalsDeepLearningWorkshop.git\r\n",
        "!cd BiosignalsDeepLearningWorkshop"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'BiosignalsDeepLearningWorkshop' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uagPEw0AU4Gx"
      },
      "source": [
        "## Reading Data\r\n",
        "\r\n",
        "The first step for the application of signal processing is to read the files that contain the data. In this case, we will use ECG files from the Fantasia dataset, that were previously downloaded and, so, can be reached using **os**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQNRMGJiUht9"
      },
      "source": [
        "# Find files in folder\r\n",
        "folder = './data/'\r\n",
        "files = os.listdir(folder)\r\n",
        "\r\n",
        "print(files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z9Spr-_VK25"
      },
      "source": [
        "import scipy.interpolate as itp\r\n",
        "\r\n",
        "def interpolate_signal(time, new_time, signal):\r\n",
        "    \"\"\"\r\n",
        "    This function uses the linear interpolates of the input signal with a certain number of samples with \"time\" \r\n",
        "    timestamp vector, according to the \"new_time\" vector.\r\n",
        "     \r\n",
        "    :param time: vector containing the the timestamps for the signal input \r\n",
        "    :param new_time: vector containing the new timestamps for interpolation\r\n",
        "    :param signal: vector with the values associated with each timestamp of the \"time\" vector\r\n",
        "    :return: \r\n",
        "        The interpolated version of the input signal for the \"new_time\" vector timestamps \r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    f = itp.interp1d(time, signal, fill_value=\"extrapolate\")\r\n",
        "    return f(new_time)\r\n",
        "\r\n",
        "\r\n",
        "def process_hr(data, quantization_size=1000):\r\n",
        "    \"\"\"\r\n",
        "        The function makes a preprocessing for the HR data and also stores the maximum and minimum values of the\r\n",
        "        HR across all records in the data dictionary. This preprocessing includes interpolation, minimum subtraction, \r\n",
        "        maximum normalization and quantization. \r\n",
        "\r\n",
        "        :param data: data dictionary containing all information\r\n",
        "        :param quantization_size: number of steps to be used in the quantization process\r\n",
        "        :return:\r\n",
        "            data: updated version of the dictionary (this may be accessed, since it is a global variable, but for a \r\n",
        "            latter publication of this code, it should be protected) \r\n",
        "            normalized_array: list containing all numpy array processed HR records \r\n",
        "    \"\"\"\r\n",
        "    # For the synchronization of both signals, I chose to interpolate the hr values.\r\n",
        "    # Since the hr values are made of a square signal the interpolation will not be far from the original\r\n",
        "    interpolated_hr = [interpolate_signal(hr_time, ppg_time, hr)\r\n",
        "                       for hr_time, ppg_time, hr in zip(data[\"hr_times\"], data[\"ppg_times\"], data[\"hr_values\"])]\r\n",
        "\r\n",
        "    \r\n",
        "    # Check if the interpolation was successfull\r\n",
        "    # for i in range(data[\"size\"]):\r\n",
        "    #    plt.plot(data[\"hr_times\"][i], normalized_hr[i], \"k\", alpha=0.7, label=\"HR\")\r\n",
        "    #    plt.plot(data[\"ppg_times\"][i], interpolated_hr[i], \"b\", alpha=0.7, label=\"HR\")\r\n",
        "    #    plt.show()\r\n",
        "\r\n",
        "    # In order to maintain the distribution of values across all subjects, I chose to normalize the data according\r\n",
        "    # to the minimum and maximum values across the all records\r\n",
        "\r\n",
        "    max = np.max(np.array([np.max(record) for record in interpolated_hr]))\r\n",
        "    min = np.min(np.array([np.min(record) for record in interpolated_hr]))\r\n",
        "    \r\n",
        "    data[\"hr_max\"] = max\r\n",
        "    data[\"hr_min\"] = min\r\n",
        "\r\n",
        "    # The minimum is removed and the set is scaled\r\n",
        "    normalized_array = [record - min for record in interpolated_hr] # Remove minimum\r\n",
        "    normalized_array = [record / max for record in normalized_array] # Normalization step\r\n",
        "\r\n",
        "    # Quantization step\r\n",
        "    # this step reduces the dimentionality of the signal, ensuring that the selected model (GRU) learns\r\n",
        "    normalized_array = [np.around(record * (quantization_size-1)).astype(int) for record in normalized_array]\r\n",
        "\r\n",
        "\r\n",
        "    return data, normalized_array\r\n",
        "\r\n",
        "    def process_ppg(records_dictionary, quantization_size=1000):\r\n",
        "    times = []\r\n",
        "    normalized_array = []\r\n",
        "    updated_ppg = []\r\n",
        "    for ppg_values, hr_time, ppg_time in \\\r\n",
        "            zip(records_dictionary[\"ppg_values\"], records_dictionary[\"hr_times\"], records_dictionary[\"ppg_times\"]):\r\n",
        "        # Since this data is not synchronized with the hr_data, we will remove the first points which does not have\r\n",
        "        # any respective value. Regarding the interpolation of the hr data, this will make the results more reliable.\r\n",
        "        first_index = np.where(ppg_time > hr_time[0])[0][0]\r\n",
        "        updated_ppg.append(ppg_values[:, first_index:])\r\n",
        "        times.append(ppg_time[first_index:])\r\n",
        "\r\n",
        "        normalized_record = ppg_values[:, first_index:]\r\n",
        "        normalized_record -= np.expand_dims(np.min(normalized_record, axis=1), -1) \\\r\n",
        "                             * np.ones((1, np.shape(normalized_record)[-1]))\r\n",
        "                             # Remove mean using algebric equation for speed performance\r\n",
        "        normalized_record = normalized_record / np.max(abs(normalized_record))  # Normalization step\r\n",
        "        #normalized_record = normalized_record / np.std(abs(normalized_record))  # Normalization step\r\n",
        "        normalized_array.append(normalized_record)\r\n",
        "\r\n",
        "    return times, normalized_array, updated_ppg"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}